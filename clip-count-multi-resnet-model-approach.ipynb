{"cells":[{"metadata":{"id":"IH_OrN_J3TNn"},"cell_type":"markdown","source":"For some reason the pre-process code for 1-25000 and master.csv was lost. (since post-processed files are stored in Google Drive)  \nThe code would be similar to what I've done to 30000-50000 and train.csv. I would try to reproduce these code for assignment delivery purpose but I can't guarantee that they would run correctly since they took too long to run.  \n(Have to admit this notebook looks messy)  \nAll code below \"#Read y\" block should work properly.The post-processed csvs (\"train2\" and \"test2\") will be listed below to show that indeed all 45000 pictures are correctly processed.  "},{"metadata":{"id":"aZTEkmjn1EOv","outputId":"e174bd21-7734-43de-c433-8068fb0093b2","executionInfo":{"status":"ok","timestamp":1586870989610,"user_tz":300,"elapsed":25798,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"try:\n    from google.colab import drive\n    drive.mount('/content/drive', force_remount=True)\n    COLAB = True\n    print(\"Note: using Google CoLab\")\n    %tensorflow_version 2.x\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n\n# Nicely formatted time string\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"G7ehdQY7l-53","outputId":"3084c3d1-e0ac-4d57-f6cc-dbf141a48926","executionInfo":{"status":"ok","timestamp":1586871033546,"user_tz":300,"elapsed":5054,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Many imports are redundant imports\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation\nfrom tensorflow.keras.layers import LeakyReLU, PReLU\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D, AveragePooling2D, ZeroPadding2D, MaxPooling2D\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom PIL import Image\nfrom tqdm.notebook import trange, tqdm\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.inception_resnet_v2 import preprocess_input # A shame we didn't try transfer learning\nfrom tensorflow.keras.applications import InceptionResNetV2 # A shame we didn't try transfer learning\n# NN-related packs end here\nimport os \nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nimport cv2\nimport shutil\nimport random\nimport math\nfrom google.colab.patches import cv2_imshow\n\nWORKING_DIR=\"/content/drive/My Drive/\"\n\nTRAIN_DATA=\"/content/drive/My Drive/train/\"\nPROCESSED_TRAIN_DATA=\"/content/drive/My Drive/proctrain/\"\n\nTEST_DATA=\"/content/drive/My Drive/test/\"\nPROCESSED_TEST_DATA=\"/content/drive/My Drive/proctest/\"\n\nnp.random.seed(42)\ntf.random.set_seed(42)\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find(\"failed\")>=0:\n  print(\"No GPU\")\nelse:\n  print(gpu_info)\n\n# Following are depreciated folders:\n# ORIGINAL_PATH=\"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/clips/\" #Folder of original image\n# PROCESSED_PATH=\"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/processed/\" #Folder of processed image\n# TRIAL_PROCESSED_PATH=\"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/grey/\" #Experimental grey images\n# RELEASE_PATH=\"/content/drive/My Drive/Colab Notebooks/Kaggle/release/clips/\"","execution_count":null,"outputs":[]},{"metadata":{"id":"JFbi2FCd1mkg","outputId":"99a0155b-76ca-4906-f5a5-2186a392d16b","executionInfo":{"status":"ok","timestamp":1586871863281,"user_tz":300,"elapsed":1269,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Create path cache to avoid I/O error.\n# Re-run multiple times until it stop giving Google Drive timeout error\n\nos.listdir(TRAIN_DATA)\nos.listdir(PROCESSED_TRAIN_DATA)\nos.listdir(TEST_DATA)\nos.listdir(PROCESSED_TEST_DATA)\n\n\n# Following are depreciated folders:\n# os.listdir('/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/')\n# os.listdir(ORIGINAL_PATH)\n# os.listdir(PROCESSED_PATH)\n# os.listdir(TRIAL_PROCESSED_PATH)\n# os.listdir(RELEASE_PATH)","execution_count":null,"outputs":[]},{"metadata":{"id":"2yuJ3N5xNv-l","trusted":false},"cell_type":"code","source":"# DEPRECIATED: Use clip.zip instead\n# # Download file of essential images, only run one time\n# # Press Ctrl + / to un-comment\n# import requests  \n# file_url = #REDACTED#\n    \n# r = requests.get(file_url, stream = True)  \n \n# with open(\"/content/drive/My Drive/Colab Notebooks/Kaggle/release/clips-data-2020.zip\", \"wb\") as file:  \n#     for block in r.iter_content(chunk_size = 1024): \n#          if block:  \n#              file.write(block)  \n\n# #Unzip, only run one time\n# !unzip -uq \"/content/drive/My Drive/Colab Notebooks/Kaggle/release/clips-data-2020.zip\" -d \"/content/drive/My Drive/Colab Notebooks/Kaggle/release/\"\n\n# # Download file of optional images, only run one time\n# # Press Ctrl + / to un-comment\n# import requests  \n# file_url = #REDACTED#\n    \n# r = requests.get(file_url, stream = True)  \n \n# with open(\"/tmp/clips-pre-data-2020.zip\", \"wb\") as file:  \n#     for block in r.iter_content(chunk_size = 1024): \n#          if block:  \n#              file.write(block)  \n\n# #Unzip, only run one time\n# !unzip -uq \"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/clips-pre-data-2020.zip\" -d \"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/\"","execution_count":0,"outputs":[]},{"metadata":{"id":"vMdIy1dArVXr","trusted":false},"cell_type":"code","source":"# DEPRECIATED: SEE NEXT BLOCK\n# # Download file of essential images, only run one time\n# # Press Ctrl + / to un-comment\n# import requests  \n# file_url = #REDACTED#\n    \n# r = requests.get(file_url, stream = True)  \n \n# with open(\"/tmp/clips-data-2020.zip\", \"wb\") as file:  \n#     for block in r.iter_content(chunk_size = 1024): \n#          if block:  \n#              file.write(block)\n\n# # Alternative unzip, during the process, DO NOT OPEN GOOGLE DRIVE, keep this window open even after the unzip process and wait\n# shutil.unpack_archive(\"/tmp/clips-data-2020.zip\",\"/tmp\")\n\n# # Download file of optional images, only run one time\n# # Press Ctrl + / to un-comment\n# import requests  \n# file_url = #REDACTED#\n    \n# r = requests.get(file_url, stream = True)  \n \n# with open(\"/tmp/clips-pre-data-2020.zip\", \"wb\") as file:  \n#     for block in r.iter_content(chunk_size = 1024): \n#          if block:  \n#              file.write(block) \n# # Alternative unzip, during the process, DO NOT OPEN GOOGLE DRIVE, keep this window open even after the unzip process and wait\n# shutil.unpack_archive(\"/tmp/clips-pre-data-2020.zip\",\"/tmp\")","execution_count":0,"outputs":[]},{"metadata":{"id":"RaCctaRjZ641","trusted":false},"cell_type":"code","source":"# Thanks to Google Drive's \"insane\" I/O, I have to manually downloaded both datasets, put clips separately into \"train\" and \"test\" folders, and re-zip and upload the zip file to the google drive.\nshutil.unpack_archive(\"/content/drive/My Drive/clips.zip\",\"/content/drive/My Drive/\")","execution_count":0,"outputs":[]},{"metadata":{"id":"6N2WTOHpgV-6","outputId":"ce16b3a1-c978-4c98-82da-bc44c23af09d","executionInfo":{"status":"ok","timestamp":1586871937113,"user_tz":300,"elapsed":37109,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Check unzip integrity, should wait for a while to run this block after unzip\nprint(\"Train Data: 1-25000, 30001-50000\")\nfor i in range(29990,50002):\n  if not os.path.exists(os.path.join(TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\nfor i in range(1,25001):\n  if not os.path.exists(os.path.join(TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n\nprint(\"Test Data: 25001-30000\")\nfor i in range(24990,30002):\n  if not os.path.exists(os.path.join(TEST_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n\n# Expected result: no 29990-30000,50001 in train data; no 24990-25000, 30001 in test data","execution_count":null,"outputs":[]},{"metadata":{"id":"f-BAiZEDR3vR","trusted":false},"cell_type":"code","source":"# Image Subtract Preprocess\n# Only need to run once\nbase=cv2.imread(os.path.join(TRAIN_DATA,\"clips-30047.png\"))\n\nraise RuntimeError # Foolproof, if you want to run, comment this line\n\nfor i in tqdm(range(1,25001)):\n  if not os.path.exists(os.path.join(TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n  else:\n    test=cv2.imread(os.path.join(TRAIN_DATA,f\"clips-{i}.png\"))\n    diff=cv2.subtract(base,test)\n    diff=cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n\n    output_filename = os.path.join((PROCESSED_TRAIN_DATA),f\"clips-{i}.png\")\n    writestatus=cv2.imwrite(output_filename,diff)\n\nfor i in tqdm(range(30000,50002)):\n  if not os.path.exists(os.path.join(TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n  else:\n    test=cv2.imread(os.path.join(TRAIN_DATA,f\"clips-{i}.png\"))\n    diff=cv2.subtract(base,test)\n    diff=cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n\n    output_filename = os.path.join((PROCESSED_TRAIN_DATA),f\"clips-{i}.png\")\n    writestatus=cv2.imwrite(output_filename,diff)","execution_count":0,"outputs":[]},{"metadata":{"id":"ZKy_VIyz71vr","outputId":"bdcc36c4-96b9-4155-cb82-f06131978fdf","executionInfo":{"status":"ok","timestamp":1585146648007,"user_tz":300,"elapsed":1849300,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Test Image Subtract\n# Only need to run once\nbase=cv2.imread(os.path.join(TRAIN_DATA,\"clips-30047.png\"))\n\nraise RuntimeError # Foolproof, if you want to run, comment this line\nfor i in tqdm(range(25001,30002)):\n  if not os.path.exists(os.path.join(TEST_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n  else:\n    test=cv2.imread(os.path.join(TEST_DATA,f\"clips-{i}.png\"))\n    diff=cv2.subtract(base,test)\n    diff=cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n\n    output_filename = os.path.join((PROCESSED_TEST_DATA),f\"clips-{i}.png\")\n    writestatus=cv2.imwrite(output_filename,diff)","execution_count":0,"outputs":[]},{"metadata":{"id":"Rh5f2U7bZamP","outputId":"7e83916c-aec8-4a8b-fd4e-604732a18e83","executionInfo":{"status":"ok","timestamp":1586745797973,"user_tz":300,"elapsed":1698,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Data frame merging and preprocess\n# Only need to run once\ndf=pd.read_csv(os.path.join(WORKING_DIR,'train.csv'))\n# Change id format to match imageDataGenerator requirement\ndf[\"id\"]=df['id'].apply(lambda x: f\"clips-{x}.png\")\ndf","execution_count":0,"outputs":[]},{"metadata":{"id":"l4UdenhM4Idg","outputId":"e59bbd70-2c52-42df-f0d7-75bafdfa20b1","executionInfo":{"status":"ok","timestamp":1586871039484,"user_tz":300,"elapsed":1779,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"df_m=pd.read_csv(os.path.join(WORKING_DIR,'master.csv'))\n# Change id format to match imageDataGenerator requirement\ndf_m[\"id\"]=df_m['id'].apply(lambda x: f\"clips-{x}.png\")\ndf_m","execution_count":null,"outputs":[]},{"metadata":{"id":"r5ccG5zEndL1","trusted":false},"cell_type":"code","source":"# Calculate average pixel\n# Only need to run once\n\nraise RuntimeError # Foolproof, if you want to run, comment this line\n\navg_pixel=[]\nfor i in tqdm(range(1,25001)):\n  if not os.path.exists(os.path.join(PROCESSED_TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n  else:\n    test=cv2.imread(os.path.join(PROCESSED_TRAIN_DATA,f\"clips-{i}.png\"),cv2.IMREAD_GRAYSCALE)\n    avg_pixel.append(np.mean(test))\ndf_avg_pixel=pd.DataFrame(avg_pixel)\ndf_m=pd.concat([df_m,df_avg_pixel],axis=1,ignore_index=True)\ndf_m=df_m.rename(columns={0:\"id\",1:\"clip_count\",2:\"avg_pixel\"})\n\navg_pixel=[]\nfor i in tqdm(range(30000,50002)):\n  if not os.path.exists(os.path.join(PROCESSED_TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n  else:\n    test=cv2.imread(os.path.join(PROCESSED_TRAIN_DATA,f\"clips-{i}.png\"),cv2.IMREAD_GRAYSCALE)\n    avg_pixel.append(np.mean(test))\ndf_avg_pixel=pd.DataFrame(avg_pixel)\ndf=pd.concat([df,df_avg_pixel],axis=1,ignore_index=True)\ndf=df.rename(columns={0:\"id\",1:\"clip_count\",2:\"avg_pixel\"})\n\ndf=pd.concat([df_m,df],axis=0,ignore_index=True)\ndf=df.sample(frac=1).reset_index(drop=True)\ndf.to_csv('/content/drive/My Drive/train2.csv',index=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"ZEhvrrs1beae","outputId":"3f3c210e-2dd4-44a2-e874-246ad8d76663","executionInfo":{"status":"ok","timestamp":1586746346260,"user_tz":300,"elapsed":1651,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Data frame preprocess\n# Only need to run once\ndf_test=pd.read_csv(os.path.join(WORKING_DIR,'test.csv'))\n# Change id format to match imageDataGenerator requirement\ndf_test[\"id\"]=df_test['id'].apply(lambda x: f\"clips-{x}.png\")\ndf_test","execution_count":0,"outputs":[]},{"metadata":{"id":"h2L6QYIEaiQi","outputId":"87db4750-7aeb-478c-a0fc-7f5940392406","executionInfo":{"status":"ok","timestamp":1585337959227,"user_tz":300,"elapsed":1271670,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Calculate average pixel\n# Only need to run once\navg_pixel_test=[]\nfor i in tqdm(range(25001,30001)):\n  if not os.path.exists(os.path.join(PROCESSED_TEST_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\n  else:\n    test=cv2.imread(os.path.join(PROCESSED_TEST_DATA,f\"clips-{i}.png\"),cv2.IMREAD_GRAYSCALE)\n    avg_pixel_test.append(np.mean(test))\ndf_avg_pixel_test=pd.DataFrame(avg_pixel_test)\ndf_test=pd.concat([df_test,df_avg_pixel_test],axis=1,ignore_index=True)\ndf_test=df_test.rename(columns={0:\"id\",1:\"avg_pixel\"})\ndf_test.to_csv('/content/drive/My Drive/test2.csv',index=False)\ndf_avg_pixel_test.describe()","execution_count":0,"outputs":[]},{"metadata":{"id":"FxNZUx_Ssbgv","outputId":"fb36e146-b9ac-4bc8-f02c-035291452777","executionInfo":{"status":"ok","timestamp":1586871359528,"user_tz":300,"elapsed":2293,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Read y\ndf=pd.read_csv(os.path.join(WORKING_DIR,'train2.csv'))\ndf_test=pd.read_csv(os.path.join(WORKING_DIR,'test2.csv'))\n\ncutoff33=df_test.quantile(q=0.33)[0]\ncutoff66=df_test.quantile(q=0.66)[0]\n\nprint(cutoff33,cutoff66)\n# 7.846227722167969 14.933803405761719\n\n# Use 30047 as base image\n# df[df[\"clip_count\"]]==0\n# # All picture's background has exact same pixel.\n# # We compare all picture pixel with one 0-clip picture, we force the pixels to be [255,255,255,255] if the pixles match\n# image1=Image.open(\"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/clips/clips-48.png\")\n# image_array1=np.asarray(image1)\n# image2=Image.open(\"/content/drive/My Drive/Colab Notebooks/Kaggle/pre_release/clips/clips-24831.png\")\n# image_array2=np.asarray(image2)\n# np.array_equal(image_array1,image_array2)\n# True\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"o-RkI8Zh5yE3","outputId":"66b2c2d4-b9e0-4ede-d968-13cdaefd7a37","executionInfo":{"status":"ok","timestamp":1586871361212,"user_tz":300,"elapsed":680,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"display(df)\n# This is to prove that all training data is used and processed\ndisplay(df_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y1ef1EwMrFw4","outputId":"9caa1d3f-2319-415f-e483-74b7c77a108d","executionInfo":{"status":"error","timestamp":1585751765342,"user_tz":300,"elapsed":87793,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# DEPRECIATED: TOO LONG RUNTIME\n# #trial picture clean\n# base = np.asarray(Image.open(os.path.join(TRAIN_DATA,\"clips-30047.png\"))).copy()\n\n# for filename in tqdm(df3[\"id\"]):\n#   test = np.asarray(Image.open(os.path.join(TRAIN_DATA,filename))).copy()\n#   for i in range(256):\n#     for j in range(256):\n#       if (np.std([test[i,j][0],test[i,j][1],test[i,j][2]])) >= 13.5 and (test[i,j][2]>test[i,j][1]) and (test[i,j][2]>test[i,j][0]):\n#         test[i,j] = np.array([255,255,255,255],dtype=\"uint8\")\n#       elif (test[i,j][0]>test[i,j][1]) and (test[i,j][0]>test[i,j][2]):\n#         if (np.std([test[i,j][0],test[i,j][1],test[i,j][2]])) >= 14 and (test[i,j][0]>test[i,j][1]) and (test[i,j][0]>test[i,j][2]):\n#           test[i,j] = np.array([255,255,255,255],dtype=\"uint8\")\n#       elif (np.std([test[i,j][0],test[i,j][1],test[i,j][2]])) >= 5 and (test[i,j][1]>test[i,j][0]) and (test[i,j][1]>test[i,j][2]):\n#         test[i,j] = np.array([255,255,255,255],dtype=\"uint8\")\n#       if np.array_equal(test[i,j],base[i,j]):\n#         test[i,j]=np.array([255,255,255,255],dtype=\"uint8\")\n\n#   diff=cv2.cvtColor(test,cv2.COLOR_BGR2GRAY)\n#   output_filename = os.path.join(PROCESSED_TRAIN_DATA,filename)\n#   output_test=Image.fromarray(diff)\n#   output_test.save(output_filename)","execution_count":0,"outputs":[]},{"metadata":{"id":"Z8B5CDkP-8Fx","outputId":"8cf97d17-5120-4a6c-d2e1-fba8c0579b8b","executionInfo":{"status":"ok","timestamp":1586872127679,"user_tz":300,"elapsed":150711,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"#Double check processed image\nfor i in range(30000,50002):\n  if not os.path.exists(os.path.join(PROCESSED_TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")\nfor i in range(0,25002):\n  if not os.path.exists(os.path.join(PROCESSED_TRAIN_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")","execution_count":null,"outputs":[]},{"metadata":{"id":"SmfGL5mn2hZJ","trusted":false},"cell_type":"code","source":"def resnet_layer(inputs,\n                 num_filters=16,\n                 kernel_size=3,\n                 strides=1,\n                 activation='relu',\n                 batch_normalization=True,\n                 conv_first=True):\n    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n\n    # Arguments\n        inputs (tensor): input tensor from input image or previous layer\n        num_filters (int): Conv2D number of filters\n        kernel_size (int): Conv2D square kernel dimensions\n        strides (int): Conv2D square stride dimensions\n        activation (string): activation name\n        batch_normalization (bool): whether to include batch normalization\n        conv_first (bool): conv-bn-activation (True) or\n            bn-activation-conv (False)\n\n    # Returns\n        x (tensor): tensor as input to the next layer\n    \"\"\"\n    conv = Conv2D(num_filters,\n                  kernel_size=kernel_size,\n                  strides=strides,\n                  padding='same',\n                  kernel_initializer='he_normal',\n                  kernel_regularizer=l2(1e-4))\n\n    x = inputs\n    if conv_first:\n        x = conv(x)\n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if activation is not None:\n            x = Activation(activation)(x)\n    else:\n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if activation is not None:\n            x = Activation(activation)(x)\n        x = conv(x)\n    return x","execution_count":0,"outputs":[]},{"metadata":{"id":"Uylaooij1lKR","trusted":false},"cell_type":"code","source":"def resnet_v2(input_shape, depth):\n    \"\"\"ResNet Version 2 Model builder [b]\n\n    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n    bottleneck layer\n    First shortcut connection per layer is 1 x 1 Conv2D.\n    Second and onwards shortcut connection is identity.\n    At the beginning of each stage, the feature map size is halved (downsampled)\n    by a convolutional layer with strides=2, while the number of filter maps is\n    doubled. Within each stage, the layers have the same number filters and the\n    same filter map sizes.\n    Features maps sizes:\n    conv1  : 32x32,  16\n    stage 0: 32x32,  64\n    stage 1: 16x16, 128\n    stage 2:  8x8,  256\n\n    # Arguments\n        input_shape (tensor): shape of input image tensor\n        depth (int): number of core convolutional layers\n        num_classes (int): number of classes (CIFAR10 has 10)\n\n    # Returns\n        model (Model): Keras model instance\n    \"\"\"\n    if (depth - 2) % 9 != 0:\n        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n    # Start model definition.\n    num_filters_in = 16\n    num_res_blocks = int((depth - 2) / 9)\n\n    inputs = Input(shape=input_shape)\n    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n    x = resnet_layer(inputs=inputs,\n                     num_filters=num_filters_in,\n                     conv_first=True)\n\n    # Instantiate the stack of residual units\n    for stage in range(3):\n        for res_block in range(num_res_blocks):\n            activation = 'relu'\n            batch_normalization = True\n            strides = 1\n            if stage == 0:\n                num_filters_out = num_filters_in * 4\n                if res_block == 0:  # first layer and first stage\n                    activation = None\n                    batch_normalization = False\n            else:\n                num_filters_out = num_filters_in * 2\n                if res_block == 0:  # first layer but not first stage\n                    strides = 2    # downsample\n\n            # bottleneck residual unit\n            y = resnet_layer(inputs=x,\n                             num_filters=num_filters_in,\n                             kernel_size=1,\n                             strides=strides,\n                             activation=activation,\n                             batch_normalization=batch_normalization,\n                             conv_first=False)\n            y = resnet_layer(inputs=y,\n                             num_filters=num_filters_in,\n                             conv_first=False)\n            y = resnet_layer(inputs=y,\n                             num_filters=num_filters_out,\n                             kernel_size=1,\n                             conv_first=False)\n            if res_block == 0:\n                # linear projection residual shortcut connection to match\n                # changed dims\n                x = resnet_layer(inputs=x,\n                                 num_filters=num_filters_out,\n                                 kernel_size=1,\n                                 strides=strides,\n                                 activation=None,\n                                 batch_normalization=False)\n            x = tensorflow.keras.layers.add([x, y])\n\n        num_filters_in = num_filters_out\n\n    # Add classifier on top.\n    # v2 has BN-ReLU before Pooling\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = AveragePooling2D(pool_size=8)(x)\n    y = Flatten()(x)\n    outputs = Dense(1,kernel_initializer='he_normal')(y)\n    # Instantiate model.\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","execution_count":0,"outputs":[]},{"metadata":{"id":"M21Ib_7J7tBi","trusted":false},"cell_type":"code","source":"def lr_schedule(epoch):\n    \"\"\"Learning Rate Schedule\n\n    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n    Called automatically every epoch as part of callbacks during training.\n\n    # Arguments\n        epoch (int): The number of epochs\n\n    # Returns\n        lr (float32): learning rate\n    \"\"\"\n    lr = 1e-3\n    if epoch > 180:\n        lr *= 0.5e-3\n    elif epoch > 160:\n        lr *= 1e-3\n    elif epoch > 120:\n        lr *= 1e-2\n    elif epoch > 80:\n        lr *= 1e-1\n    print('Learning rate: ', lr)\n    return lr","execution_count":0,"outputs":[]},{"metadata":{"id":"Um39UmspVpYD","trusted":false},"cell_type":"code","source":"# Separate training images into 3 parts: [0-33],(33-66],(66-100]\ndf1=df[(df.avg_pixel<=cutoff33)]\ndf2=df[(df.avg_pixel>cutoff33)&(df.avg_pixel<=cutoff66)]\ndf3=df[(df.avg_pixel>cutoff66)]\n","execution_count":0,"outputs":[]},{"metadata":{"id":"oO17fjX3EK3B","outputId":"940314f2-feb3-4a7d-9be0-9db6cc32e638","executionInfo":{"status":"ok","timestamp":1586533164674,"user_tz":300,"elapsed":296,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"print(df1.shape[0],df2.shape[0],df3.shape[0])\n# 15325 14582 15093\n#df1: 12260:3065\n#df2: 11666:2916\n#df3: 12075:3018","execution_count":0,"outputs":[]},{"metadata":{"id":"2xuYHex95Lim","trusted":false},"cell_type":"code","source":"# Model parameters (prefer not to change)\nBATCH_SIZE = 32  # orig paper trained all networks with batch_size=128\nEPOCHS = 200\nCOLORS = 1\nDEPTH = COLORS * 9 + 2\ninput_shape=[256,256,COLORS]","execution_count":0,"outputs":[]},{"metadata":{"id":"zx5SL5vkU6jV"},"cell_type":"markdown","source":"How is the validation split computed?  \n\nIf you set the validation_split argument in model.fit to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed."},{"metadata":{"id":"YmMzVlxbhSAT","outputId":"32b8fd7d-de9d-442f-dbf7-fc90e0244832","executionInfo":{"status":"ok","timestamp":1586460473273,"user_tz":300,"elapsed":9364,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Ignore this block, just use model=load_model('/content/drive/My Drive/Resnet_0408latest_2q.hdf5')\n# The following code didn't solve the issue of missing optimizer status\n\n###########\n# # Run this if you want to load model\n# # Initialize model (remember to re-define generators after this step)\n# lr_scheduler_load = LearningRateScheduler(lr_schedule)\n\n# lr_reducer_load = ReduceLROnPlateau(factor=np.sqrt(0.1),\n#                                cooldown=0,\n#                                patience=5,\n#                                min_lr=0.5e-6)\n# datagen=ImageDataGenerator(rescale=1./255,validation_split=0.5)\n# callbacks_load=[lr_scheduler_load,lr_reducer_load]\n# train_gen=datagen.flow_from_dataframe(\n#     dataframe=df3[:64],\n#     directory='/content/drive/My Drive/proctrain',\n#     x_col=\"id\",\n#     y_col=\"clip_count\",\n#     target_size=(256,256),\n#     color_mode=\"grayscale\",\n#     class_mode=\"raw\",\n#     batch_size=BATCH_SIZE,\n#     shuffle=True,\n#     subset=\"training\",\n#     seed=42\n# )\n# val_gen=datagen.flow_from_dataframe(\n#     dataframe=df3[:64],\n#     directory='/content/drive/My Drive/proctrain',\n#     x_col=\"id\",\n#     y_col=\"clip_count\",\n#     target_size=(256,256),\n#     color_mode=\"grayscale\",\n#     class_mode=\"raw\",\n#     batch_size=BATCH_SIZE,\n#     shuffle=True,\n#     subset=\"validation\",\n#     seed=42)\n\n# model=resnet_v2(input_shape=input_shape,depth=DEPTH)\n# model.compile(loss='mean_squared_error',\n#               optimizer=Adam(learning_rate=lr_schedule(0)),\n#               metrics=['accuracy'])\n\n# model.fit(x=train_gen,\n#           epochs=2,\n#           initial_epoch=0, # Change based on training model\n#           callbacks=callbacks_load,\n#           validation_data=val_gen,\n#           steps_per_epoch=1,\n#           validation_steps=1,\n#           workers=1,\n#           use_multiprocessing=False)\n# model.load_weights('/content/drive/My Drive/Resnet_0408latest_2q.hdf5')","execution_count":0,"outputs":[]},{"metadata":{"id":"pig3EZdH7nV9","outputId":"0d4e74d0-8262-4e61-ae72-08f376df6ec4","executionInfo":{"status":"ok","timestamp":1586294554405,"user_tz":300,"elapsed":32771749,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"#1st quantile\n\ntrain_datagen=ImageDataGenerator(rescale=1./255,\n                horizontal_flip=True,\n                vertical_flip=True)\n\ntrain_gen=train_datagen.flow_from_dataframe(\n    dataframe=df1[:12260],\n    directory=PROCESSED_TRAIN_DATA,\n    x_col=\"id\",\n    y_col=\"clip_count\",\n    target_size=(256,256),\n    color_mode=\"grayscale\",\n    class_mode=\"raw\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42\n)\n\nval_datagen=ImageDataGenerator(rescale=1./255)\n\nval_gen=val_datagen.flow_from_dataframe(\n    dataframe=df1[12260:],\n    directory=PROCESSED_TRAIN_DATA,\n    x_col=\"id\",\n    y_col=\"clip_count\",\n    target_size=(256,256),\n    color_mode=\"grayscale\",\n    class_mode=\"raw\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42)\nSTEPS=train_gen.n/train_gen.batch_size\nVAL_STEPS=val_gen.n/val_gen.batch_size\n\nmodel=resnet_v2(input_shape=input_shape,depth=DEPTH)\nmodel.compile(loss='mean_squared_error',\n              optimizer=Adam(learning_rate=lr_schedule(0)),\n              metrics=['accuracy'])\n\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=5,\n                               min_lr=0.5e-6)\nmodel_checkpoint=ModelCheckpoint(f'/content/drive/My Drive/Resnet_0407best-1q.hdf5',verbose=1,monitor='val_loss',save_best_only=True,mode=\"min\")# For best model\nmodel_checkpoint2=ModelCheckpoint(f'/content/drive/My Drive/Resnet_0407latest-1q.hdf5',verbose=0,monitor='val_loss',save_best_only=False,mode=\"min\")# For resume training\n\ncallbacks = [lr_reducer, lr_scheduler, model_checkpoint, model_checkpoint2]\n\nmodel.fit(x=train_gen,\n          epochs=EPOCHS,\n          initial_epoch=0, # Change based on new model or resume training\n          callbacks=callbacks,\n          validation_data=val_gen,\n          steps_per_epoch=STEPS,\n          validation_steps=VAL_STEPS,\n          workers=1,\n          use_multiprocessing=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"a41wHs6xYe16","outputId":"6608607e-1e17-4e85-a173-38523ba2dea2","executionInfo":{"status":"ok","timestamp":1586468314610,"user_tz":300,"elapsed":7777375,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# 2nd quantile\ntrain_datagen=ImageDataGenerator(rescale=1./255,\n                horizontal_flip=True,\n                vertical_flip=True)\n\ntrain_gen=train_datagen.flow_from_dataframe(\n    dataframe=df2[:11666],\n    directory=PROCESSED_TRAIN_DATA,\n    x_col=\"id\",\n    y_col=\"clip_count\",\n    target_size=(256,256),\n    color_mode=\"grayscale\",\n    class_mode=\"raw\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=46 # DO NOT KEEP THE SAME SEED IF RESUME TRAINING\n)\n\nval_datagen=ImageDataGenerator(rescale=1./255)\n\nval_gen=val_datagen.flow_from_dataframe(\n    dataframe=df2[11666:],\n    directory=PROCESSED_TRAIN_DATA,\n    x_col=\"id\",\n    y_col=\"clip_count\",\n    target_size=(256,256),\n    color_mode=\"grayscale\",\n    class_mode=\"raw\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=46 # DO NOT KEEP THE SAME SEED IF RESUME TRAINING\n)\nSTEPS=train_gen.n/train_gen.batch_size\nVAL_STEPS=val_gen.n/val_gen.batch_size\n\n# model=resnet_v2(input_shape=input_shape,depth=DEPTH)\n# model.compile(loss='mean_squared_error',\n#               optimizer=Adam(learning_rate=lr_schedule(0)),\n#               metrics=['accuracy'])\n\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=5,\n                               min_lr=0.5e-6)\nmodel_checkpoint=ModelCheckpoint(f'/content/drive/My Drive/Resnet_0408best_2q.hdf5',verbose=1,monitor='val_loss',save_best_only=True,mode=\"min\")# For best model\nmodel_checkpoint2=ModelCheckpoint(f'/content/drive/My Drive/Resnet_0408latest_2q.hdf5',verbose=0,monitor='val_loss',save_best_only=False,mode=\"min\")# For resume training\n\ncallbacks = [lr_reducer, lr_scheduler, model_checkpoint, model_checkpoint2]\n\nmodel.fit(x=train_gen,\n          epochs=EPOCHS,\n          initial_epoch=148, # Change based on new model or resume training\n          callbacks=callbacks,\n          validation_data=val_gen,\n          steps_per_epoch=STEPS,\n          validation_steps=VAL_STEPS,\n          workers=1,\n          use_multiprocessing=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"24KbUJjcIOMQ","outputId":"e619a805-e196-42b8-e2bc-ea839e6899e8","executionInfo":{"status":"ok","timestamp":1586557622658,"user_tz":300,"elapsed":24423515,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# 3rd quantile\ntrain_datagen=ImageDataGenerator(rescale=1./255,\n                horizontal_flip=True,\n                vertical_flip=True)\n\ntrain_gen=train_datagen.flow_from_dataframe(\n    dataframe=df3[:12075],\n    directory=PROCESSED_TRAIN_DATA,\n    x_col=\"id\",\n    y_col=\"clip_count\",\n    target_size=(256,256),\n    color_mode=\"grayscale\",\n    class_mode=\"raw\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42\n)\n\nval_datagen=ImageDataGenerator(rescale=1./255)\n\nval_gen=val_datagen.flow_from_dataframe(\n    dataframe=df3[12075:],\n    directory=PROCESSED_TRAIN_DATA,\n    x_col=\"id\",\n    y_col=\"clip_count\",\n    target_size=(256,256),\n    color_mode=\"grayscale\",\n    class_mode=\"raw\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=42)\nSTEPS=train_gen.n/train_gen.batch_size\nVAL_STEPS=val_gen.n/val_gen.batch_size\n\nmodel=resnet_v2(input_shape=input_shape,depth=DEPTH)\nmodel.compile(loss='mean_squared_error',\n              optimizer=Adam(learning_rate=lr_schedule(0)),\n              metrics=['accuracy'])\n\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=5,\n                               min_lr=0.5e-6)\nmodel_checkpoint=ModelCheckpoint(f'/content/drive/My Drive/Resnet_0410best_3q.hdf5',verbose=1,monitor='val_loss',save_best_only=True,mode=\"min\")# For best model\nmodel_checkpoint2=ModelCheckpoint(f'/content/drive/My Drive/Resnet_0410latest_3q.hdf5',verbose=0,monitor='val_loss',save_best_only=False,mode=\"min\")# For resume training\n\ncallbacks = [lr_reducer, lr_scheduler, model_checkpoint, model_checkpoint2]\n\nmodel.fit(x=train_gen,\n          epochs=EPOCHS,\n          initial_epoch=0, # Change based on new model or resume training\n          callbacks=callbacks,\n          validation_data=val_gen,\n          steps_per_epoch=STEPS,\n          validation_steps=VAL_STEPS,\n          workers=1,\n          use_multiprocessing=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"mzAtkgoQHzYF","outputId":"1b058700-b463-42d8-ce02-e3d0eb7e474d","executionInfo":{"status":"ok","timestamp":1585442988264,"user_tz":300,"elapsed":2536,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Prediction creation step start here\n#Double check processed image, shold only report 30001 not found\nfor i in range(25001,30002):\n  if not os.path.exists(os.path.join(PROCESSED_TEST_DATA,f\"clips-{i}.png\")):\n    print(f\"Error: clips {i} not found\")","execution_count":0,"outputs":[]},{"metadata":{"id":"Huu-ITPL7qMl","outputId":"157dc401-22d9-4671-8c9d-d35af4ba3bb8","executionInfo":{"status":"ok","timestamp":1586557631273,"user_tz":300,"elapsed":397,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"# Test dataframe modify (load)\ndf_test=pd.read_csv(os.path.join(WORKING_DIR,'test2.csv'))\n#df_test[\"id\"]=df_test['id'].apply(lambda x: f\"clips-{x}.png\")\ndf_test","execution_count":0,"outputs":[]},{"metadata":{"id":"qhBBvYl6XC0X","outputId":"1c369676-4618-4955-f8db-cc83d0a0396a","executionInfo":{"status":"ok","timestamp":1586557637857,"user_tz":300,"elapsed":488,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"print(cutoff33,cutoff66)\n# 7.846227722167969 14.933803405761719\ndf_test_1q=df_test[df_test.avg_pixel<=cutoff33].reset_index(drop=True)\ndf_test_2q=df_test[(df_test.avg_pixel>cutoff33)&(df_test.avg_pixel<=cutoff66)].reset_index(drop=True)\ndf_test_3q=df_test[(df_test.avg_pixel>cutoff66)].reset_index(drop=True)\nprint(df_test_1q.shape[0],df_test_2q.shape[0],df_test_3q.shape[0])","execution_count":0,"outputs":[]},{"metadata":{"id":"l5cQtjm8WmXu","trusted":false},"cell_type":"code","source":"#Quantile Prediction\ndef quant_pred(model_path,df_quantile):\n  model_quant=load_model(model_path)\n  print(f\"Loaded model from: {model_path}\")\n  test_datagen=ImageDataGenerator(rescale=1./255)\n  test_gen=test_datagen.flow_from_dataframe(\n      dataframe=df_quantile,\n      directory='/content/drive/My Drive/proctest',\n      x_col=\"id\",\n      target_size=(256,256),\n      color_mode=\"grayscale\",\n      class_mode=None,\n      batch_size=1,\n      shuffle=False)\n  test_steps=test_gen.n/test_gen.batch_size\n  test_gen.reset()\n  print(f\"Predicting {df_quantile.shape[0]} number of obs:\")\n  pred=model_quant.predict(test_gen,verbose=1,steps=test_steps)\n  pred.flatten()\n  pred=pd.DataFrame(pred)\n  print(pred.shape)\n  return pred","execution_count":0,"outputs":[]},{"metadata":{"id":"lxxtjUBzXpeY","outputId":"28e42cc5-f629-48e7-fe23-456461b7f384","executionInfo":{"status":"ok","timestamp":1586559377726,"user_tz":300,"elapsed":1734487,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"pred_1q=quant_pred('/content/drive/My Drive/Resnet_0407best-1q.hdf5',df_test_1q)\npred_2q=quant_pred('/content/drive/My Drive/Resnet_0408best_2q.hdf5',df_test_2q)\npred_3q=quant_pred('/content/drive/My Drive/Resnet_0410best_3q.hdf5',df_test_3q)","execution_count":0,"outputs":[]},{"metadata":{"id":"llSx2WrCZmf-","outputId":"effcdaaf-69f3-4ab6-b95c-5c381d3cfd4d","executionInfo":{"status":"ok","timestamp":1586559378043,"user_tz":300,"elapsed":296,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"df_test_1q=pd.concat([df_test_1q,pred_1q],axis=1,ignore_index=True)\ndf_test_2q=pd.concat([df_test_2q,pred_2q],axis=1,ignore_index=True)\ndf_test_3q=pd.concat([df_test_3q,pred_3q],axis=1,ignore_index=True)\n\nsubmit_df=pd.concat([df_test_1q,df_test_2q,df_test_3q],axis=0,ignore_index=True)\nsubmit_df=submit_df.rename(columns={0:\"id\",1:\"avg_pixel\",2:\"clip_count\"})\nsubmit_df[\"id\"]=submit_df[\"id\"].str.strip(\"clips-.png\")\nsubmit_df=submit_df.sort_values(by=\"id\").reset_index(drop=True).drop(columns=\"avg_pixel\")\nsubmit_df","execution_count":0,"outputs":[]},{"metadata":{"id":"kCFiadefPyRf","outputId":"0247ca54-6d22-4348-c412-eed0ae56f980","executionInfo":{"status":"ok","timestamp":1586561119667,"user_tz":300,"elapsed":1741904,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"#Hard-code check if two pictures are identical, if identical, set clip count to 0\nbase=cv2.imread(os.path.join(TRAIN_DATA,\"clips-30047.png\"))\nchanged=0\nfor index,row in tqdm(submit_df.iterrows()):\n  image_id=int(row[0])\n  test=cv2.imread(os.path.join(TEST_DATA,f\"clips-{image_id}.png\"))\n  if (np.array_equal(base,test)==True):\n    submit_df.loc[index,\"clip_count\"]=0\n    changed+=1\nprint(changed)","execution_count":0,"outputs":[]},{"metadata":{"id":"eQB4bZxL6Jau","outputId":"31cd5b7c-d269-4703-d1cc-0e535e5feb59","executionInfo":{"status":"ok","timestamp":1585961827982,"user_tz":300,"elapsed":380,"user":{"displayName":"PlaneWalker","photoUrl":"","userId":"15316050097517350725"}},"trusted":false},"cell_type":"code","source":"submit_df[submit_df[\"clip_count\"]==0]","execution_count":0,"outputs":[]},{"metadata":{"id":"8Kyu9SP9Q334","trusted":false},"cell_type":"code","source":"submit_df.to_csv('/content/drive/My Drive/Resnet0410_threesplit.csv',index=False)","execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"Mid-term_delivery.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"995f288198754448a379b89dc40823a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2db0cb9c4b7c4f9395685d8f42887b70","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_351ca7ba43574d8c8ea3a4f04f9dfc40","IPY_MODEL_76f24a8aad964764947a22b11257d409"]}},"2db0cb9c4b7c4f9395685d8f42887b70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"351ca7ba43574d8c8ea3a4f04f9dfc40":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_44110703f28e477989335d8d7d3f11e8","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":5001,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5001,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71d274e02b2447898b9f357d0110b26e"}},"76f24a8aad964764947a22b11257d409":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_280b79f3b3fb40b7a7fdf5e6dfc7d34e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5001/5001 [36:43&lt;00:00,  2.27it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e03f3aa8c04a407cb943298093f8f20f"}},"44110703f28e477989335d8d7d3f11e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"71d274e02b2447898b9f357d0110b26e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"280b79f3b3fb40b7a7fdf5e6dfc7d34e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e03f3aa8c04a407cb943298093f8f20f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c4a385388dfb4137a4b5b606d10f9fbb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_17cd2a60263746b0bcfb9e7b1e125a1f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_424b82dfa6be42179ed4758cac7ff1f2","IPY_MODEL_34a05a0a615b4dd49c45b3719234975f"]}},"17cd2a60263746b0bcfb9e7b1e125a1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"424b82dfa6be42179ed4758cac7ff1f2":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c2a8d0b360a54fbfa9a70508d655d08c","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":5000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fb2dad57897a461ebfdbce3b8c7e9eb3"}},"34a05a0a615b4dd49c45b3719234975f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_05f0ace6442541c9bd634eb87d315417","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5000/5000 [21:11&lt;00:00,  3.93it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8e19759433484b0ebec8178655125db9"}},"c2a8d0b360a54fbfa9a70508d655d08c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fb2dad57897a461ebfdbce3b8c7e9eb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05f0ace6442541c9bd634eb87d315417":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8e19759433484b0ebec8178655125db9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d6b8ce433e0474581ebace45c19201c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fd0ff8a15c7446d1b125648895a7306e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d23c32b824ef413e9e039a5695340d99","IPY_MODEL_ab9e691eb36c4425815bfadb4cecea1c"]}},"fd0ff8a15c7446d1b125648895a7306e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d23c32b824ef413e9e039a5695340d99":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_38629a8b5bcf460680b14360e834aee3","_dom_classes":[],"description":"  0%","_model_name":"IntProgressModel","bar_style":"danger","max":11250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":14,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8424342c3424cd19911c85cadec787a"}},"ab9e691eb36c4425815bfadb4cecea1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cde8c57feaf24077abec124119a89c7a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 14/11250 [01:27&lt;18:49:05,  6.03s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4ad2c7557fdb4428ba6a68820c986d2d"}},"38629a8b5bcf460680b14360e834aee3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b8424342c3424cd19911c85cadec787a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cde8c57feaf24077abec124119a89c7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4ad2c7557fdb4428ba6a68820c986d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"552a602b94b040818875834730f54e45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8086347bafa945af9f88f14f25c62588","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b4e8e178e3374c14b96392d5f17f4713","IPY_MODEL_de70b69a1c7c4fd4bda16d41b49fa5f2"]}},"8086347bafa945af9f88f14f25c62588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4e8e178e3374c14b96392d5f17f4713":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6814061a4914459ca48305799c43b03e","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78722e255b1240458ae706087b7f6445"}},"de70b69a1c7c4fd4bda16d41b49fa5f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2e8f8303ee3c401b964b92493cfd2abc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5000/? [29:00&lt;00:00,  2.87it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5619d138f0d4e0aa6b295aae1053f72"}},"6814061a4914459ca48305799c43b03e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"78722e255b1240458ae706087b7f6445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e8f8303ee3c401b964b92493cfd2abc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b5619d138f0d4e0aa6b295aae1053f72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}